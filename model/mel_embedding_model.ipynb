{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mel_embedding_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNazFKqx+0A7o5fep07d9wQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- Causal Dilated Convolution Autoencoder을 활용하여 멜로디의 유사성을 반영할 수 있는 노래의 임베딩을 구함\n","\n","- 멜 스펙트로그램 데이터를 압축하여 멜로디의 유사성을 반영할 수 있는 노래의 임베딩을 구하는 것이 본 모델의 목표\n","\n","- 모델의 구조는 다음과 같음\n","    - Mel-Spectrogram-> Causal Dilated Convolution Encoder -> Layer -> Tanh -> Layer -> Causal Dilated Convolution Decoder -> Reconstruct Mel-Spectrogram\n","    - Causal Dilated Convolution Encoder는 Causal padding과 Dilated Convolution을 활용하여 적은 Layer로도 t - 1의 인과성을 효과적으로 학습할 수 았도록 구성함 (과거의 데이터를 학습하여 데이터를 압축)\n","    - Layer -> Tanh -> Layer는 Mel-Spectrogram을 -1 ~ 1 까지의 값을 가지는 128차원의 Mel 임베등으로 나타낼 수 있도록 구성함\n","    - Causal Dilated Convolution Decoder는 Reverse 함수, Causal padding, Dilated Convolution을 활용하여 적은 Layer로도 t + 1의 인과성을 효과적으로 학습할 수 았도록 구성함 (미래의 데이터를 학습하여 과거를 복원)\n","    - Mel-Spectrogram과 Reconstruct Mel-Spectrogram의 MSELoss를 바탕으로 모델의 가중치를 업데이트 함\n","\n","\n","- 모델 학습 결과 동일한 노래의 일본어, 한국어 버전 등의 노래가 가장 유사하게 나오는 것으로 보아 본 모델이 멜로디의 유사성을 반영했다고 볼 수 있음"],"metadata":{"id":"3FrPlfs783BE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0mYPhYac19jq"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import math\n","import pickle\n","import gc\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","\n","data_dir = '/content/drive/MyDrive/제 13회 투빅스 컨퍼런스 음악추천/Data/'\n","batch_data_dir = '/content/drive/MyDrive/제 13회 투빅스 컨퍼런스 음악추천/Batch_Data/'\n","model_dir = '/content/drive/MyDrive/제 13회 투빅스 컨퍼런스 음악추천/Model/'"]},{"cell_type":"markdown","source":["# 데이터 확인"],"metadata":{"id":"1sRBib872z8W"}},{"cell_type":"code","source":["# 노래 데이터 불러오기\n","song_meta_df = pd.read_json(data_dir + 'song_meta_data_v3.json')\n","song_meta_df = song_meta_df.sort_values('id')\n","song_meta_df = song_meta_df.reset_index(drop = True)\n","song_meta_df['song_embedding_idx'] = song_meta_df.index"],"metadata":{"id":"jj4bbgYX21TS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 배치 단위 데이터 불러오기\n","import re\n","import os\n","file_list = os.listdir(batch_data_dir)\n","\n","def get_int(x):\n","    x = int(re.sub('[^0-9]', '', x))\n","    return x\n","\n","file_list = sorted([get_int(x) for x in file_list], reverse = True)"],"metadata":{"id":"AzOrOa7z23XV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_name = file_list[0]\n","batch_li = [i for i in range(1, batch_name + 1)]\n","random.Random(22).shuffle(batch_li)\n","\n","train_batch_li = batch_li[:-100]\n","val_batch_li = batch_li[-100:]\n","test_batch_li = sorted(batch_li)"],"metadata":{"id":"f3-2Uwzd28qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnoRYekXpSCr"},"source":["# 학습 설정"]},{"cell_type":"code","metadata":{"id":"B2aDDEnxhyT4"},"source":["epochs = 500\n","batch_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_puBDPmhrop"},"source":["if torch.cuda.is_available():\n","  DEVICE = torch.device('cuda')\n","else:\n","  DEVICE = torch.device('cpu')\n","print(DEVICE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiS_GEPZEgGo"},"source":["def train(model, train_loader):\n","    model.train()\n","    train_loss = 0\n","\n","    for batch_name in tqdm(train_loader):\n","        mel = np.load(batch_data_dir + f'{batch_name}.npy')\n","\n","        mel = torch.FloatTensor(mel).to(DEVICE)\n","        \n","        optimizer.zero_grad()\n","        \n","        encode, output = model(mel)\n","        \n","        loss = criterion(output, mel)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n"," \n","    train_loss /= (len(train_loader))\n","\n","    return train_loss\n","\n","def val(model, train_loader):\n","    model.eval()\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for batch_name in tqdm(train_loader):\n","            mel = np.load(batch_data_dir + f'{batch_name}.npy')\n","            \n","            mel = torch.FloatTensor(mel).to(DEVICE)\n","\n","            encode, output = model(mel)\n","\n","            loss = criterion(output, mel)\n","\n","            val_loss += loss.item()\n","\n","    val_loss /= (len(train_loader))\n","\n","    return val_loss\n","\n","\n","def get_mel_embeding(model, train_loader):\n","    model.eval()\n","    mel_embeding_li = []\n","    with torch.no_grad():\n","        for batch_name in tqdm(train_loader):\n","            mel = np.load(batch_data_dir + f'{batch_name}.npy')\n","            \n","            mel = torch.FloatTensor(mel).to(DEVICE)\n","            encode, output = model(mel)\n","            mel_embeding_li.append(encode.detach().cpu().numpy())\n","\n","    return mel_embeding_li"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oEyFY3JUeIn"},"source":["# encoder <- 과거의 데이터를 바탕으로 미래를 예측 (t-1의 인과성 학습)\n","# decoder <- 미래의 데이터를 바탕으로 과거를 예측 (t+1의 인과성 학습)\n","class TimeAutoEncoder(nn.Module):\n","    def __init__(self):\n","        super(TimeAutoEncoder, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(in_channels = 48, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 1),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv1d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv3 = nn.Sequential(\n","            nn.Conv1d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv4 = nn.Sequential(\n","            nn.Conv1d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv5 = nn.Sequential(\n","            nn.Conv1d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv6 = nn.Sequential(\n","            nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n","            nn.BatchNorm1d(16),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv7 = nn.Sequential(\n","            nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n","            nn.BatchNorm1d(8),\n","            nn.ReLU(),\n","        )\n","\n","        self.encoder_fc = nn.Sequential(\n","            nn.Linear(8 * 1876, 128),\n","            nn.BatchNorm1d(128),\n","            nn.Tanh(),\n","        )\n","        \n","        self.decoder_fc = nn.Sequential(\n","            nn.Linear(128, 8 * 1876),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv1 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 8, out_channels = 16, kernel_size  = 3, stride = 1, dilation=62),\n","            nn.Conv1d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 0, dilation = 64),\n","            nn.BatchNorm1d(16),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv2 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 16, out_channels = 32, kernel_size  = 3, stride = 1, dilation = 30),\n","            nn.Conv1d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 0, dilation = 32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv3 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 32, out_channels = 64, kernel_size  = 3, stride = 1, dilation=14),\n","            nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 0, dilation = 16),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv4 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 64, out_channels = 128, kernel_size  = 3, stride = 1, dilation = 6),\n","            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 0, dilation = 8),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv5 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 128, out_channels = 256, kernel_size  = 3, stride = 1, dilation=2),\n","            nn.Conv1d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 0, dilation = 4),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv6 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 256, out_channels = 512, kernel_size  = 3, stride = 1, dilation = 1),\n","            nn.Conv1d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 0, dilation = 2),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(),\n","        )\n","\n","        self.t_conv7 = nn.Sequential(\n","            # nn.ConvTranspose1d(in_channels = 512, out_channels = 48, kernel_size  = 3, stride = 1, dilation= 1),\n","            nn.Conv1d(in_channels = 512, out_channels = 48, kernel_size = 3, stride = 1, padding = 0, dilation = 1)\n","        )\n","\n","    def forward(self, mel_spec):\n","        x = F.pad(mel_spec, pad = (2, 0, 0, 0))\n","        x = self.conv1(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (4, 0, 0, 0))\n","        x = self.conv2(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (8, 0, 0, 0))\n","        x = self.conv3(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (16, 0, 0, 0))\n","        x = self.conv4(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (32, 0, 0, 0))\n","        x = self.conv5(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (64, 0, 0, 0))\n","        x = self.conv6(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (128, 0, 0, 0))\n","        x = self.conv7(x)\n","        # print(x.shape)\n","        encode = self.encoder_fc(x.view(-1, 8 * 1876))\n","\n","        # print('decode')\n","        x = self.decoder_fc(encode)\n","        x = x.view(-1, 8, 1876)\n","        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n","        x = F.pad(x, pad = (128, 0, 0, 0))\n","        x = self.t_conv1(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (64, 0, 0, 0))\n","        x = self.t_conv2(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (32, 0, 0, 0))\n","        x = self.t_conv3(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (16, 0, 0, 0))\n","        x = self.t_conv4(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (8, 0, 0, 0))\n","        x = self.t_conv5(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (4, 0, 0, 0))\n","        x = self.t_conv6(x)\n","        # print(x.shape)\n","        x = F.pad(x, pad = (2, 0, 0, 0))\n","        x = self.t_conv7(x)\n","        # print(x.shape)\n","        x = torch.swapaxes(torch.fliplr(torch.swapaxes(x, 1, 2)), 1, 2)\n","        \n","        return encode, x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습"],"metadata":{"id":"4XeK7c7rkH15"}},{"cell_type":"code","metadata":{"id":"7R_VocfAYh-M"},"source":["model = TimeAutoEncoder().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","criterion = nn.MSELoss()\n","\n","# print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zgabAv3YvOkU","executionInfo":{"status":"ok","timestamp":1640163559404,"user_tz":-540,"elapsed":534,"user":{"displayName":"이성범","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11166281350151773159"}},"outputId":"637da1b6-45e2-4f56-c3c4-374905516221"},"source":["from torchsummary import summary as summary_\n","\n","summary_(model, (48, 1876),batch_size = batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv1d-1           [128, 512, 1876]          74,240\n","       BatchNorm1d-2           [128, 512, 1876]           1,024\n","              ReLU-3           [128, 512, 1876]               0\n","            Conv1d-4           [128, 256, 1876]         393,472\n","       BatchNorm1d-5           [128, 256, 1876]             512\n","              ReLU-6           [128, 256, 1876]               0\n","            Conv1d-7           [128, 128, 1876]          98,432\n","       BatchNorm1d-8           [128, 128, 1876]             256\n","              ReLU-9           [128, 128, 1876]               0\n","           Conv1d-10            [128, 64, 1876]          24,640\n","      BatchNorm1d-11            [128, 64, 1876]             128\n","             ReLU-12            [128, 64, 1876]               0\n","           Conv1d-13            [128, 32, 1876]           6,176\n","      BatchNorm1d-14            [128, 32, 1876]              64\n","             ReLU-15            [128, 32, 1876]               0\n","           Conv1d-16            [128, 16, 1876]           1,552\n","      BatchNorm1d-17            [128, 16, 1876]              32\n","             ReLU-18            [128, 16, 1876]               0\n","           Conv1d-19             [128, 8, 1876]             392\n","      BatchNorm1d-20             [128, 8, 1876]              16\n","             ReLU-21             [128, 8, 1876]               0\n","           Linear-22                 [128, 128]       1,921,152\n","      BatchNorm1d-23                 [128, 128]             256\n","             Tanh-24                 [128, 128]               0\n","           Linear-25               [128, 15008]       1,936,032\n","             ReLU-26               [128, 15008]               0\n","           Conv1d-27            [128, 16, 1876]             400\n","      BatchNorm1d-28            [128, 16, 1876]              32\n","             ReLU-29            [128, 16, 1876]               0\n","           Conv1d-30            [128, 32, 1876]           1,568\n","      BatchNorm1d-31            [128, 32, 1876]              64\n","             ReLU-32            [128, 32, 1876]               0\n","           Conv1d-33            [128, 64, 1876]           6,208\n","      BatchNorm1d-34            [128, 64, 1876]             128\n","             ReLU-35            [128, 64, 1876]               0\n","           Conv1d-36           [128, 128, 1876]          24,704\n","      BatchNorm1d-37           [128, 128, 1876]             256\n","             ReLU-38           [128, 128, 1876]               0\n","           Conv1d-39           [128, 256, 1876]          98,560\n","      BatchNorm1d-40           [128, 256, 1876]             512\n","             ReLU-41           [128, 256, 1876]               0\n","           Conv1d-42           [128, 512, 1876]         393,728\n","      BatchNorm1d-43           [128, 512, 1876]           1,024\n","             ReLU-44           [128, 512, 1876]               0\n","           Conv1d-45            [128, 48, 1876]          73,776\n","================================================================\n","Total params: 5,059,336\n","Trainable params: 5,059,336\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 43.97\n","Forward/backward pass size (MB): 11241.72\n","Params size (MB): 19.30\n","Estimated Total Size (MB): 11304.99\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"XnBLD7bQhxTZ"},"source":["import time\n","\n","min_loss = 987654321\n","\n","for epoch in range(1, epochs + 1):\n","    start = time.time()\n","    random.shuffle(train_batch_li)\n","    train_loss = train(model = model, train_loader = train_batch_li)\n","    val_loss = val(model = model, train_loader = val_batch_li)\n","    end = time.time()\n","\n","    print(f'EPOCH:{epoch}, Train Loss:{train_loss}, Val Loss:{val_loss}, 학습 시간: {end - start}')\n","    if val_loss < min_loss:\n","        min_loss = val_loss\n","        torch.save(model.state_dict(), model_dir + f'TimeAutoEncoder_val.pt')\n","        print('모델 저장')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hV8uSgHZpN3M"},"source":["# mel_embedding 저장"]},{"cell_type":"code","metadata":{"id":"h43GNYyQ9PeE"},"source":["model = TimeAutoEncoder().to(DEVICE)\n","model.load_state_dict(torch.load(model_dir + f'TimeAutoEncoder_val.pt', map_location = DEVICE))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hb2JWUe-2EgZ"},"source":["mel_embeding_li = get_mel_embeding(model = model, train_loader = test_batch_li)\n","mel_embeding = np.concatenate(mel_embeding_li, axis = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mel_embeding.shape"],"metadata":{"id":"SWptJ2ynWmwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save(batch_data_dir + 'mel_embeding_val.npy', mel_embeding)"],"metadata":{"id":"bcfBBGXQznX7"},"execution_count":null,"outputs":[]}]}